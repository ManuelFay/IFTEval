{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "# df = pd.read_csv('../outputs/results_cross.csv').sort_values(by=['model', 'target_class', 'num_samples'])\n",
    "df = pd.read_csv('../outputs/results_exp1a_cross.csv').sort_values(by=['model', 'target_class', 'num_samples'])\n",
    "excluded = [\"conll\", \"mnli\", \"qnli\", \"squadv2\", \"sst2\", \"stsb\", \"xsum\", \"squad\"]\n",
    "cols = [\"gpt-4_lm_score\", \"gpt-3.5-turbo_lm_score\"]\n",
    "for col in cols:\n",
    "    df[col] = df[col]/10\n",
    "# df2.mean_score_pred = df2.mean_scor\n",
    "df.target_class = df.target_class.apply(lambda x: x.replace(\"fractial_\", \"\"))\n",
    "df = df[df.model.apply(lambda x: x not in excluded)]\n",
    "df.num_samples = df.num_samples.apply(lambda x: int(x))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_1000 = df[df.num_samples == 1000][[\"model\", \"target_class\", \"gpt-4_lm_score\"]].pivot(index=\"model\", columns=\"target_class\", values=\"gpt-4_lm_score\")\n",
    "\n",
    "# Plot a heatmap\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "sns.heatmap(heatmap_1000, annot=True, fmt=\".2f\", cmap=\"coolwarm\", ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_0 = df[df.num_samples == 0][[\"model\", \"target_class\", \"gpt-4_lm_score\"]].pivot(index=\"model\", columns=\"target_class\", values=\"gpt-4_lm_score\")\n",
    "\n",
    "# Plot a heatmap\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "sns.heatmap(heatmap_0, annot=True, fmt=\".2f\", cmap=\"coolwarm\", ax=ax)\n",
    "plt.savefig(\"../outputs/plots/exp1a_heatmap_heldout.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a heatmap\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "sns.heatmap((heatmap_1000 - heatmap_0).fillna(0), annot=True, fmt=\".2f\", cmap=\"Spectral\", ax=ax)\n",
    "plt.xlabel = \"Target class\"\n",
    "plt.ylabel = \"Delta Models\"\n",
    "plt.savefig(\"../outputs/plots/exp1a_heatmap_diff.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = [\"gpt-4_lm_score\", \"gpt-3.5-turbo_lm_score\", \"rouge1\", \"bertscore\", \"sbertscore\", \"softmaxed_reward_model_score\"] # , \"rougeL\"]\n",
    "excluded = [\"conll\", \"mnli\", \"qnli\", \"squadv2\", \"sst2\", \"stsb\", \"xsum\", \"squad_v2\"]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df), len(df.target_class.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df.model.apply(lambda x: x  not in excluded)) & (df.target_class.apply(lambda x: x not in excluded))].groupby(\"target_class\")[target_col].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df[(df.model.apply(lambda x: x  not in excluded)) & (df.target_class.apply(lambda x: x not in excluded))].groupby(\"target_class\")[target_col].corr(method=\"spearman\").reset_index().groupby(\"level_1\")[target_col].mean().reindex(target_col)[target_col] * 100\n",
    "matrix = np.triu(corr, k=1)\n",
    "\n",
    "display(corr)\n",
    "# plot the heatmap\n",
    "assert (corr.columns == corr.index).all()\n",
    "clean_cols = [\"GPT4\", \"GPT3.5\", \"ROUGE\", \"BScore\", \"SBERT\", \"RM\"] # , \"ROUGE-L\"]\n",
    "print(corr.columns, clean_cols )\n",
    "corr.index.name = \"\"\n",
    "plt.rcParams.update({'font.size': 22})\n",
    "ax = sns.heatmap(corr,\n",
    "        xticklabels=clean_cols,\n",
    "        yticklabels=clean_cols,\n",
    "            mask=matrix, annot=True, fmt=\".0f\", vmin=0, vmax=100,\n",
    "            cmap=\"coolwarm\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=0)\n",
    "\n",
    "for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] +\n",
    "             ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "    item.set_fontsize(22)\n",
    "\n",
    "plt.savefig(\"../outputs/plots/exp1a_corr.pdf\", bbox_inches='tight')\n",
    "plt.rcParams.update({'font.size': 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df.groupby([\"model\", \"num_samples\"]).count()))\n",
    "exp1a = df[df.target_class.apply(lambda x: x not in excluded)][df.num_samples < 3000].groupby([\"target_class\"])[target_col].mean().sort_values(\"gpt-4_lm_score\", ascending=False)\n",
    "exp1a.columns = [\"GPT-4\", \"GPT-3.5\", \"ROUGE-1\", \"BERTScore\", \"SBERT\", \"RM\", \"Soft RM\", \"ROUGE-L\"]\n",
    "exp1a.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp1a[[\"ROUGE-1\", \"GPT-4\", \"GPT-3.5\", \"BERTScore\", \"SBERT\", \"RM\", \"Soft RM\"]].transpose()[[\"logic\", \"code\", \"rewrite\", \"extract\", \"memoryanswer\",\"write\"]].apply(lambda x: round(x,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(exp1a[[\"ROUGE-1\", \"GPT-4\", \"GPT-3.5\", \"BERTScore\", \"SBERT\", \"RM\", \"Soft RM\"]].transpose()[[\"logic\", \"code\", \"rewrite\", \"extract\", \"memoryanswer\",\"write\"]].apply(lambda x: round(x,2)).to_latex(float_format=\"%.2f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(exp1a.to_latex(float_format=\"%.2f\"))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "excluded = [\"conll\", \"mnli\", \"qnli\", \"squadv2\", \"sst2\", \"stsb\", \"wnli\", \"xsum\"]\n",
    "for cat in df.target_class.unique():\n",
    "    if cat in excluded:\n",
    "        continue\n",
    "    sns.lineplot(x=\"num_samples\", y=\"mean_score_pred\", data=df[(df.model == cat) & (df.target_class == cat)].sort_values(by=\"num_samples\"), label=cat)\n",
    "# set x-axis to log scale\n",
    "plt.xscale('symlog')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "excluded = [\"conll\", \"mnli\", \"qnli\", \"squadv2\", \"sst2\", \"stsb\", \"wnli\", \"xsum\"]\n",
    "for cat in df.target_class.unique():\n",
    "    if cat in excluded:\n",
    "        continue\n",
    "    sns.lineplot(x=\"num_samples\", y=\"rougeL\", data=df[(df.model == cat) & (df.target_class == cat)].sort_values(by=\"num_samples\"), label=cat)\n",
    "# set x-axis to log scale\n",
    "plt.xscale('symlog')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "excluded = [\"conll\", \"mnli\", \"qnli\", \"squadv2\", \"sst2\", \"stsb\", \"wnli\", \"xsum\"]\n",
    "for cat in df.target_class.unique():\n",
    "    if not cat in excluded:\n",
    "        continue\n",
    "    sns.lineplot(x=\"num_samples\", y=\"mean_score_pred\", data=df[(df.model == cat) & (df.target_class == cat)].sort_values(by=\"num_samples\"), label=cat)\n",
    "# set x-axis to log scale\n",
    "plt.xscale('symlog')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plt.show()\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "excluded = [\"conll\", \"mnli\", \"qnli\", \"squadv2\", \"sst2\", \"stsb\", \"wnli\", \"xsum\"]\n",
    "for cat in df.target_class.unique():\n",
    "    if not cat in excluded:\n",
    "        continue\n",
    "    sns.lineplot(x=\"num_samples\", y=\"exact_match\",\n",
    "                 data=df[(df.model == cat) & (df.target_class == cat)].sort_values(by=\"num_samples\"), label=cat)\n",
    "# set x-axis to log scale\n",
    "plt.xscale('symlog')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plt.show()\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "excluded = [\"conll\", \"mnli\", \"qnli\", \"squadv2\", \"sst2\", \"stsb\", \"wnli\", \"xsum\"]\n",
    "for cat in df.target_class.unique():\n",
    "    if not cat in excluded:\n",
    "        continue\n",
    "    sns.lineplot(x=\"num_samples\", y=\"custom_score\",\n",
    "                 data=df[(df.model == cat) & (df.target_class == cat)].sort_values(by=\"num_samples\"), label=cat)\n",
    "\n",
    "# set x-axis to log scale\n",
    "plt.xscale('symlog')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1B: Custom datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.lines import Line2D\n",
    "line1 = Line2D([0], [0], label='Exact Match', color='k')\n",
    "line2 = Line2D([0], [0], label='Custom Score', color='k', linestyle='dashed')\n",
    "line3 = Line2D([0], [0], label='GPT-4 Score', color='k', linestyle='dotted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# df2 = pd.read_csv('../outputs/results_standard.csv').sort_values(by=['model', 'num_samples'])\n",
    "df2 = pd.read_csv('../outputs/results_exp1b_standard_datasets.csv').sort_values(by=['model', 'num_samples'])\n",
    "# df2 = pd.read_csv('../outputs/results_exp1b_standard_datasets_2023-05-15.csv').sort_values(by=['model', 'num_samples'])\n",
    "# tmp = pd.read_csv('../outputs/results_exp1b_standard_datasets_backup.csv')\n",
    "# df2 = pd.concat([df2, tmp[tmp.model == \"xsum\"]]).fillna(0)\n",
    "\n",
    "df2[\"target_class\"] = df2.model\n",
    "# df2 = df2[df2.model != \"squadv2\"]\n",
    "cols = [\"gpt-4_lm_score\", \"gpt-3.5-turbo_lm_score\"]\n",
    "for col in cols:\n",
    "    df2[col] = df2[col]/10\n",
    "# df2.mean_score_pred = df2.mean_score_pred/10\n",
    "# df2.mean_score_ref = df2.mean_score_ref/10\n",
    "df2 = df2[df2.num_samples < 5000]\n",
    "df2 = df2[df2.file.apply(lambda x: \"mnli_0_0\" not in x )]\n",
    "df2 = df2[df2.file.apply(lambda x: \"mnli_0_1\" not in x )]\n",
    "# df2.loc[df2.model == \"conll\", \"custom_score\"] = df2[df2.model == \"conll\"].apply(lambda x: x.custom_score*0.5  + x.rouge1*0.5, axis=1)\n",
    "df2.num_samples = df2.num_samples.apply(lambda x: int(x) if x != \"only\" else 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2col = {\n",
    "    \"conll\": \"red\",\n",
    "    \"mnli\": \"blue\",\n",
    "    \"sst2\": \"yellow\",\n",
    "    \"qnli\": \"green\",\n",
    "    \"squadv2\": \"orange\",\n",
    "    # \"stsb\": \"black\",\n",
    "    \"xsum\": \"gray\"\n",
    "}\n",
    "target_col = [\"rouge1\",\n",
    "              \"gpt-4_lm_score\",\n",
    "              # \"gpt-4_mean_score_ratio\",\n",
    "              \"gpt-3.5-turbo_lm_score\",\n",
    "              \"bertscore\",\n",
    "              \"exact_match\",\n",
    "              \"sbertscore\",\n",
    "              \"reward_model_score\",\n",
    "              \"softmaxed_reward_model_score\",\n",
    "              \"custom_score\"]\n",
    "\n",
    "fig, axs = plt.subplots(ncols=3, nrows=2)\n",
    "fig.tight_layout()\n",
    "\n",
    "for i, cat in enumerate(label2col.keys()):\n",
    "    data_df = df2[(df2.model == cat) & (df2.target_class == cat)].sort_values(by=\"num_samples\")\n",
    "    # data_df = data_df.groupby([\"target_class\", \"num_samples\"])[target_col].mean().reset_index()\n",
    "    sns.lineplot(x=\"num_samples\", y=\"exact_match\",\n",
    "                 data=data_df, label=cat, color=label2col[cat], ax=axs[i//3][i%3])\n",
    "\n",
    "    sns.lineplot(x=\"num_samples\", y=\"custom_score\",\n",
    "                 data=data_df, linestyle='dashed', color=label2col[cat], ax=axs[i//3][i%3])\n",
    "    sns.lineplot(x=\"num_samples\", y=\"gpt-4_lm_score\",\n",
    "                 data=df2[(df2.model == cat) & (df2.target_class == cat)].sort_values(by=\"num_samples\"), linestyle='dotted', color=label2col[cat], ax=axs[i//3][i%3])\n",
    "\n",
    "    axs[i//3][i%3].set_xscale('symlog')\n",
    "    axs[i//3][i%3].set(xlabel=cat, ylabel='')\n",
    "    axs[i//3][i%3].get_legend().remove()\n",
    "\n",
    "\n",
    "df2_save = df2.copy()\n",
    "    # ax2.get_legend().remove()\n",
    "\n",
    "# where some data has already been plotted to ax\n",
    "# handles, labels = ax2.get_legend_handles_labels()\n",
    "# handles.extend([line1, line2])\n",
    "handles= [line1, line2, line3]\n",
    "plt.legend(handles=handles, loc='upper left')\n",
    "# plot the legend\n",
    "# ax1.set(xlabel='# of training samples', ylabel='Mean Score')\n",
    "\n",
    "\n",
    "# set x-axis to log scale\n",
    "plt.xscale('symlog')\n",
    "plt.ylim((0, 1))\n",
    "plt.savefig(\"../outputs/plots/exp1b_1.pdf\", bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "fig, axs = plt.subplots(ncols=3, nrows=2)\n",
    "fig.tight_layout()\n",
    "\n",
    "for i, cat in enumerate(label2col.keys()):\n",
    "\n",
    "    sns.lineplot(x=\"num_samples\", y=\"custom_score\",\n",
    "                 data=df2[(df2.model == cat) & (df2.target_class == cat)].sort_values(by=\"num_samples\"), label=cat, linestyle='dashed', color=label2col[cat], ax=axs[i//3][i%3])\n",
    "\n",
    "    sns.lineplot(x=\"num_samples\", y=\"gpt-4_lm_score\",\n",
    "                 data=df2[(df2.model == cat) & (df2.target_class == cat)].sort_values(by=\"num_samples\"), linestyle='dotted', color=label2col[cat], ax=axs[i//3][i%3])\n",
    "    axs[i//3][i%3].set_xscale('symlog')\n",
    "    axs[i//3][i%3].set(xlabel=cat, ylabel='')\n",
    "    axs[i//3][i%3].get_legend().remove()\n",
    "\n",
    "# ax1.set(xlabel='# of training samples', ylabel='Mean Score')\n",
    "# handles, labels = ax2.get_legend_handles_labels()\n",
    "# handles.extend([line3, line2])\n",
    "# plt.legend(handles=handles, loc='upper left')\n",
    "handles= [line3, line2]\n",
    "plt.legend(handles=handles, loc='upper left')\n",
    "\n",
    "# set x-axis to log scale\n",
    "plt.ylim((0, 1))\n",
    "plt.savefig(\"../outputs/plots/exp1b_2.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axs.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "label2col = {\n",
    "    \"conll\": \"red\",\n",
    "    \"mnli\": \"blue\",\n",
    "    \"qnli\": \"green\",\n",
    "    \"squadv2\": \"orange\",\n",
    "    \"sst2\": \"yellow\",\n",
    "    \"stsb\": \"black\",\n",
    "    \"xsum\": \"gray\"\n",
    "}\n",
    "target_col = [\"rouge1\",\n",
    "              \"gpt-4_lm_score\",\n",
    "              # \"gpt-4_mean_score_ratio\",\n",
    "              \"gpt-3.5-turbo_lm_score\",\n",
    "              \"bertscore\",\n",
    "              \"exact_match\",\n",
    "              \"sbertscore\",\n",
    "              \"reward_model_score\",\n",
    "              \"softmaxed_reward_model_score\",\n",
    "              \"custom_score\"]\n",
    "\n",
    "\n",
    "for cat in df2.target_class.unique():\n",
    "    print(cat)\n",
    "    plt.figure()\n",
    "    data_df = df2[(df2.model == cat) & (df2.target_class == cat)].sort_values(by=\"num_samples\")\n",
    "    # data_df = data_df.groupby([\"target_class\", \"num_samples\"])[target_col].mean().reset_index()\n",
    "    if  not cat in label2col.keys():\n",
    "        continue\n",
    "    ax1 = sns.scatterplot(x=\"custom_score\", y=\"exact_match\", size=\"num_samples\", hue=\"num_samples\", legend=\"full\",\n",
    "                 data=data_df)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # ax2.get_legend().remove()\n",
    "\n",
    "# where some data has already been plotted to ax\n",
    "# handles, labels = ax2.get_legend_handles_labels()\n",
    "# handles.extend([line1, line2])\n",
    "#\n",
    "# # plot the legend\n",
    "# plt.legend(handles=handles, loc='upper left')\n",
    "ax1.set(xlabel='# of training samples', ylabel='Mean Score')\n",
    "\n",
    "\n",
    "# set x-axis to log scale\n",
    "plt.xscale('symlog')\n",
    "plt.ylim((0, 1))\n",
    "plt.savefig(\"../outputs/plots/exp1b_1.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = [\"rouge1\",\n",
    "              \"gpt-4_lm_score\",\n",
    "              # \"gpt-4_mean_score_ratio\",\n",
    "              \"gpt-3.5-turbo_lm_score\",\n",
    "              # \"gpt-3.5-turbo_mean_score_ratio\",\n",
    "              \"bertscore\",\n",
    "              # \"exact_match\",\n",
    "              \"sbertscore\",\n",
    "              # \"reward_model_score\",\n",
    "              \"softmaxed_reward_model_score\",\n",
    "              \"custom_score\"]\n",
    "\n",
    "clean_target_col = [\"ROUGE-1\",\n",
    "              \"GPT4\",\n",
    "              # \"GPT-4 Ratio\",\n",
    "              \"GPT3.5\",\n",
    "              # \"GPT-3.5 Ratio\",\n",
    "              \"BScore\",\n",
    "              # \"EM\",\n",
    "              \"SBert\",\n",
    "              # \"RM\",\n",
    "              \"RM\",\n",
    "              \"Human\"]\n",
    "df2[target_col].corr(method=\"spearman\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.sample(10)[[\"rouge1\", \"custom_score\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# excluded = [\"conll\", \"mnli\", \"qnli\", \"squadv2\", \"sst2\", \"stsb\", \"xsum\"]\n",
    "excluded = [\"conll\", \"mnli\", \"qnli\", \"sst2\", \"stsb\", \"squadv2\"]\n",
    "# excluded = [\"conll\"]\n",
    "corr = df2[(df2.model.apply(lambda x: x  in excluded)) & (df2.target_class.apply(lambda x: x in excluded))][target_col].corr(method=\"spearman\")\n",
    "matrix = np.triu(corr)\n",
    "\n",
    "display(corr)\n",
    "# plot the heatmap\n",
    "sns.heatmap(corr,\n",
    "        xticklabels=corr.columns,\n",
    "        yticklabels=corr.columns,\n",
    "            annot=True, mask=matrix, vmin=0, vmax=1,\n",
    "            cmap=\"coolwarm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# excluded = [\"conll\", \"mnli\", \"qnli\", \"squadv2\", \"sst2\", \"stsb\", \"xsum\"]\n",
    "excluded = [\"conll\", \"mnli\", \"qnli\", \"sst2\", \"stsb\", \"squadv2\"]\n",
    "# excluded = [\"conll\"]\n",
    "corr = df2[(df2.model.apply(lambda x: x  in excluded)) & (df2.target_class.apply(lambda x: x in excluded)) & (df2.num_samples<100)][target_col].corr(method=\"spearman\").reindex(target_col)[target_col]\n",
    "matrix = np.triu(corr, k=1)\n",
    "# plot the heatmap\n",
    "assert (corr.columns == corr.index).all()\n",
    "print(corr.columns, clean_target_col )\n",
    "corr.index.name = \"\"\n",
    "\n",
    "# display(corr)\n",
    "# plot the heatmap\n",
    "sns.heatmap(corr,\n",
    "        xticklabels=clean_target_col,\n",
    "        yticklabels=clean_target_col,\n",
    "            annot=True, mask=matrix, vmin=0, vmax=1,\n",
    "            cmap=\"coolwarm\")\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=0)\n",
    "\n",
    "plt.savefig(\"../outputs/plots/exp1b_metrics_corr_total.pdf\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "corr = df2[(df2.model.apply(lambda x: x  in excluded)) & (df2.target_class.apply(lambda x: x in excluded)) & (df2.num_samples<100)].groupby(\"target_class\")[target_col].corr(method=\"spearman\").reset_index().groupby(\"level_1\")[target_col].mean().reindex(target_col)[target_col]\n",
    "matrix = np.triu(corr, k=1)\n",
    "# plot the heatmap\n",
    "assert (corr.columns == corr.index).all()\n",
    "print(corr.columns, clean_target_col )\n",
    "corr.index.name = \"\"\n",
    "sns.heatmap(corr,\n",
    "        xticklabels=clean_target_col,\n",
    "        yticklabels=clean_target_col,\n",
    "            annot=True, mask=matrix, vmin=0, vmax=1,\n",
    "            cmap=\"coolwarm\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=0)\n",
    "plt.savefig(\"../outputs/plots/exp1b_metrics_corr_mean.pdf\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# excluded = [\"conll\", \"mnli\", \"qnli\", \"squadv2\", \"sst2\", \"stsb\", \"xsum\"]\n",
    "excluded = [\"conll\", \"mnli\", \"qnli\", \"sst2\", \"stsb\", \"squadv2\"]\n",
    "# excluded = [\"conll\"]\n",
    "corr = df2[(df2.model.apply(lambda x: x  in excluded)) & (df2.target_class.apply(lambda x: x in excluded)) & (df2.num_samples==0)][target_col].corr(method=\"spearman\").reindex(target_col)[target_col]\n",
    "matrix = np.triu(corr, k=1)\n",
    "# plot the heatmap\n",
    "assert (corr.columns == corr.index).all()\n",
    "print(corr.columns, clean_target_col )\n",
    "corr.index.name = \"\"\n",
    "\n",
    "# display(corr)\n",
    "# plot the heatmap\n",
    "sns.heatmap(corr,\n",
    "        xticklabels=clean_target_col,\n",
    "        yticklabels=clean_target_col,\n",
    "            annot=True, mask=matrix, vmin=0, vmax=1,\n",
    "            cmap=\"coolwarm\")\n",
    "plt.savefig(\"../outputs/plots/exp1b_metrics_corr_total.pdf\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "corr = df2[(df2.model.apply(lambda x: x  in excluded)) & (df2.target_class.apply(lambda x: x in excluded)) & (df2.num_samples==0)].groupby(\"target_class\")[target_col].corr(method=\"spearman\").reset_index().groupby(\"level_1\")[target_col].mean().reindex(target_col)[target_col]\n",
    "matrix = np.triu(corr, k=1)\n",
    "# plot the heatmap\n",
    "assert (corr.columns == corr.index).all()\n",
    "print(corr.columns, clean_target_col )\n",
    "corr.index.name = \"\"\n",
    "sns.heatmap(corr,\n",
    "        xticklabels=clean_target_col,\n",
    "        yticklabels=clean_target_col,\n",
    "            annot=True, mask=matrix, vmin=0, vmax=1,\n",
    "            cmap=\"coolwarm\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.savefig(\"../outputs/plots/exp1b_metrics_corr_mean.pdf\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df2[(df2.model.apply(lambda x: x  in excluded)) & (df2.target_class.apply(lambda x: x in excluded)) & (df2.num_samples==0)].groupby(\"target_class\")[target_col].corr(method=\"spearman\").reset_index().groupby(\"level_1\")[target_col].count()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "target_col = [\"rouge1\",\n",
    "              \"gpt-4_lm_score\",\n",
    "              \"gpt-3.5-turbo_lm_score\",\n",
    "              \"bertscore\",\n",
    "              \"sbertscore\",\n",
    "              \"reward_model_score\",\n",
    "              \"softmaxed_reward_model_score\",\n",
    "              \"custom_score\"\n",
    "              ]\n",
    "\n",
    "excluded = [\"sst2\", \"mnli\", \"qnli\"]\n",
    "\n",
    "a = df2[(df2.model.apply(lambda x: x  in excluded)) & (df2.target_class.apply(lambda x: x in excluded)) & (df2.num_samples==0)].groupby(\"target_class\")[target_col].mean()\n",
    "\n",
    "b = df2[(df2.model.apply(lambda x: x  in excluded)) & (df2.target_class.apply(lambda x: x in excluded)) & (df2.num_samples==1000)].groupby(\"target_class\")[target_col].mean()\n",
    "\n",
    "b"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ref = (((b-a)/a).mean(axis=0)[\"custom_score\"])\n",
    "\n",
    "((b-a)/a).mean(axis=0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xsum only\n",
    "target_col = [\"rouge1\",\n",
    "              \"gpt-4_lm_score\",\n",
    "              \"gpt-4_mean_score_ratio\",\n",
    "              \"gpt-3.5-turbo_lm_score\",\n",
    "              \"bertscore\",\n",
    "              \"sbertscore\",\n",
    "              \"reward_model_score\",\n",
    "              \"softmaxed_reward_model_score\"\n",
    "              ]# excluded = [\"conll\", \"mnli\", \"qnli\", \"squadv2\", \"sst2\", \"stsb\", \"xsum\"]\n",
    "excluded = [\"xsum\"]\n",
    "# excluded = [\"conll\"]\n",
    "corr = df2[(df2.model.apply(lambda x: x  in excluded)) & (df2.target_class.apply(lambda x: x in excluded)) & (df2.num_samples>-1)][target_col].corr(method=\"spearman\").reindex(target_col)[target_col]\n",
    "matrix = np.triu(corr)\n",
    "\n",
    "# display(corr)\n",
    "# plot the heatmap\n",
    "sns.heatmap(corr,\n",
    "        xticklabels=corr.columns,\n",
    "        yticklabels=corr.index,\n",
    "            annot=True, mask=matrix, vmin=0, vmax=1,\n",
    "            cmap=\"coolwarm\")\n",
    "plt.show()& (df2.num_samples<100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[(df2.model.apply(lambda x: x  in excluded)) & (df2.target_class.apply(lambda x: x in excluded)) & (df2.num_samples<50)].groupby(\"target_class\")[target_col].corr(method=\"pearson\").reset_index().groupby(\"level_1\")[target_col].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = [\"rouge1\",\n",
    "              \"gpt-4_lm_score\",\n",
    "              # \"gpt-4_mean_score_ratio\",\n",
    "              \"gpt-3.5-turbo_lm_score\",\n",
    "              \"bertscore\",\n",
    "              \"exact_match\",\n",
    "              \"sbertscore\",\n",
    "              \"reward_model_score\",\n",
    "              \"softmaxed_reward_model_score\",\n",
    "              \"custom_score\"]\n",
    "\n",
    "excludeds = [\"conll\", \"mnli\", \"qnli\", \"squadv2\", \"sst2\", \"stsb\", \"xsum\"]\n",
    "for excluded in excludeds:\n",
    "    corr = df2[(df2.model.apply(lambda x: x  == excluded)) & (df2.target_class.apply(lambda x: x == excluded)) & (df2.num_samples==1000)][target_col].corr(method=\"spearman\")\n",
    "    matrix = np.triu(corr)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "\n",
    "    # display(corr)\n",
    "    print(\"Excluded:\", excluded)\n",
    "    # plot the heatmap\n",
    "    sns.heatmap(corr,\n",
    "            xticklabels=corr.columns,\n",
    "            yticklabels=corr.columns,\n",
    "                annot=True, mask=matrix,\n",
    "                cmap=\"coolwarm\")\n",
    "    # plt.savefig(f\"../outputs/plots/exp1b_3_{excluded}.pdf\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.groupby(\"target_class\")[target_col].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df2[df2.target_class==\"xsum\"][\"gpt-3.5-turbo_responses\"].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=\"custom_score\", y=\"gpt-4_lm_score\", data=df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_counts = df2.groupby(['custom_score', 'gpt-4_lm_score']).size().reset_index(name='count')\n",
    "plt.figure()\n",
    "sns.scatterplot(df_counts, x='custom_score', y='gpt-4_lm_score', size=\"count\", sizes=(50, 200))\n",
    "\n",
    "plt.figure()\n",
    "sns.kdeplot(data=df_counts,  x='custom_score', y='gpt-4_lm_score', fill=True, levels=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from importlib import reload\n",
    "plt=reload(plt)\n",
    "\n",
    "# df2 = pd.read_csv('../outputs/results_standard.csv').sort_values(by=['model', 'num_samples'])\n",
    "df = pd.read_csv('../outputs/results_exp1b_synthetic_datasets.csv').sort_values(by=['model', 'num_samples'])\n",
    "# df2 = pd.read_csv('../outputs/results_exp1b_standard_datasets_2023-05-15.csv').sort_values(by=['model', 'num_samples'])\n",
    "# tmp = pd.read_csv('../outputs/results_exp1b_standard_datasets_backup.csv')\n",
    "# df2 = pd.concat([df2, tmp[tmp.model == \"xsum\"]]).fillna(0)\n",
    "\n",
    "datasets  = [\"sst2\", \"mnli\", \"conll\"]\n",
    "\n",
    "def task_mapper(x):\n",
    "    task = mapper(x)\n",
    "    task_map = {\n",
    "        f\"{task}\": \"Manual\",\n",
    "        f\"synth{task}\": \"Synthetic\",\n",
    "        f\"synth{task}rand\": \"Random\",\n",
    "        f\"synth{task}bootstrapped\": \"Bootstrapped\",\n",
    "\n",
    "    }\n",
    "    return task_map[x]\n",
    "\n",
    "def mapper(x):\n",
    "    if \"sst2\" in x:\n",
    "        return \"sst2\"\n",
    "    if \"conll\" in x:\n",
    "        return \"conll\"\n",
    "    if \"mnli\" in x:\n",
    "        return \"mnli\"\n",
    "    print(x)\n",
    "    raise ValueError\n",
    "\n",
    "df[\"model\"] = df.task.apply(lambda x: mapper(x))\n",
    "df[\"task\"]= df.task.apply(lambda x: task_mapper(x))\n",
    "\n",
    "df[\"target_class\"] = df.model\n",
    "# df2 = df2[df2.model != \"squadv2\"]\n",
    "cols = [\"gpt-4_lm_score\"]\n",
    "for col in cols:\n",
    "    df[col] = df[col]/10\n",
    "# df2.mean_score_pred = df2.mean_score_pred/10\n",
    "# df2.mean_score_ref = df2.mean_score_ref/10\n",
    "\n",
    "fig, axs = plt.subplots(ncols=3, nrows=2, figsize=(6, 3))\n",
    "fig.tight_layout()\n",
    "\n",
    "\n",
    "label2col = {\n",
    "    \"Manual\": \"black\",\n",
    "    \"Synthetic\": \"red\",\n",
    "    \"Random\": \"blue\",\n",
    "    \"Bootstrapped\": \"green\",\n",
    "}\n",
    "\n",
    "for i, model in enumerate(datasets):\n",
    "    df2 = df[df.model == model]\n",
    "    base = df2[(df2.num_samples == 0) & (df2.task != \"Bootstrapped\")][\"gpt-4_lm_score\"].mean()\n",
    "    base_boot = df2[(df2.num_samples == 0) & (df2.task == \"Bootstrapped\")][\"exact_match\"].mean()\n",
    "    yline = 100 if model != \"conll\" else 50\n",
    "    print(base, base_boot)\n",
    "\n",
    "    for task in df2.task.unique():\n",
    "        tmp_df = df2[df2.task == task]\n",
    "        # tmp_df[\"gpt-4_lm_score\"] = smooth(list(tmp_df[\"gpt-4_lm_score\"].values), 0.5)\n",
    "\n",
    "        if task==\"Bootstrapped\":\n",
    "            axs[0][i].axhline(y=base, xmin=0.0, xmax=1000, color='black', linestyle=\"dashdot\", alpha=0.3)\n",
    "            axs[0][i].axvline(x=yline if not \"conll\" else 50, ymin=0.0, ymax=1.0, color='black', linestyle=\"dashdot\", alpha=0.3)\n",
    "        # axs[0][i].text(s=\"qqq\", x=0, y= 0.5)\n",
    "\n",
    "        tmp_df.loc[tmp_df.num_samples == 0, \"gpt-4_lm_score\"] = base\n",
    "        axs[0][i] = sns.lineplot(data=tmp_df, x=\"num_samples\", y=\"gpt-4_lm_score\", color=label2col[task], legend=None, errorbar=None, ax=axs[0][i]) #gpt-4_lm_score\"\n",
    "        # ax = sns.lineplot(data=df2, x=\"num_samples\", y=\"exact_match\", hue=\"task\", linestyle=\"dashed\", errorbar=None)\n",
    "\n",
    "        # y axis\n",
    "        axs[0][i].set_ylim(0.5, 1)\n",
    "        axs[0][i].set_xlim(0, 1000)\n",
    "\n",
    "        axs[0][i].set_xscale('symlog')\n",
    "        axs[0][i].set(xlabel=\"\", ylabel=\"GPT4 Score\" if i==0 else \"\")\n",
    "\n",
    "        if task==\"Bootstrapped\":\n",
    "            axs[1][i].axhline(y=base_boot, xmin=0.0, xmax=1000, color='black', linestyle=\"dashdot\", alpha=0.3)\n",
    "            axs[1][i].axvline(x=yline, ymin=0.0, ymax=1.0, color='black', linestyle=\"dashdot\", alpha=0.3)\n",
    "\n",
    "        axs[1][i] = sns.lineplot(data=tmp_df, x=\"num_samples\", y=\"exact_match\", color=label2col[task], legend=None, errorbar=None, ax=axs[1][i]) #gpt-4_lm_score\"\n",
    "        # ax = sns.lineplot(data=df2, x=\"num_samples\", y=\"exact_match\", hue=\"task\", linestyle=\"dashed\", errorbar=None)\n",
    "\n",
    "        # y axis\n",
    "        axs[1][i].set_xlim(0, 1000)\n",
    "        axs[1][i].set_ylim(-0.02, 1)\n",
    "\n",
    "        axs[1][i].set_xscale('symlog')\n",
    "        axs[1][i].set(xlabel=r'$N$', ylabel=\"Exact Match\" if i==0 else \"\")\n",
    "        axs[0][i].set_title(model)\n",
    "\n",
    "        def font_up(ax):\n",
    "            for item in ([ax.title, ax.xaxis.label, ax.yaxis.label]):\n",
    "                    item.set_fontsize(13)\n",
    "        font_up(axs[0][i])\n",
    "        font_up(axs[1][i])\n",
    "\n",
    "\n",
    "handles, labels = axs[0][0].get_legend_handles_labels()\n",
    "from matplotlib.lines import Line2D\n",
    "line1 = Line2D([0], [0], label='H', color='black')\n",
    "line2 = Line2D([0], [0], label='S', color='red')\n",
    "line3 = Line2D([0], [0], label='R', color='blue')\n",
    "line4 = Line2D([0], [0], label='S+H', color='green')\n",
    "handles.extend([line1, line2, line3, line4])\n",
    "axs[0][0].legend(fontsize=14, handles=handles, loc='lower left', bbox_to_anchor=(0.2, 1.13), ncol=4, borderaxespad=0, frameon=False)\n",
    "# Legend font size\n",
    "# plt.legend(loc='lower left')\n",
    "\n",
    "plt.savefig(\"../outputs/plots/sym_exp1.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.task == \"Manual\"][[\"gpt-4_lm_score\", \"num_samples\", \"model\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# df2 = pd.read_csv('../outputs/results_standard.csv').sort_values(by=['model', 'num_samples'])\n",
    "df2 = pd.read_csv('../outputs/results_exp1b_standard_datasets_neg.csv').sort_values(by=['model', 'num_samples'])\n",
    "# df2 = pd.read_csv('../outputs/results_exp1b_standard_datasets_2023-05-15.csv').sort_values(by=['model', 'num_samples'])\n",
    "# tmp = pd.read_csv('../outputs/results_exp1b_standard_datasets_backup.csv')\n",
    "# df2 = pd.concat([df2, tmp[tmp.model == \"xsum\"]]).fillna(0)\n",
    "\n",
    "df2[\"target_class\"] = df2.model\n",
    "# df2 = df2[df2.model != \"squadv2\"]\n",
    "cols = [\"gpt-4_lm_score\"]\n",
    "for col in cols:\n",
    "    df2[col] = df2[col]/10\n",
    "# df2.mean_score_pred = df2.mean_score_pred/10\n",
    "# df2.mean_score_ref = df2.mean_score_ref/10\n",
    "# df2.loc[df2.model == \"conll\", \"custom_score\"] = df2[df2.model == \"conll\"].apply(lambda x: x.custom_score*0.5  + x.rouge1*0.5, axis=1)\n",
    "df2.num_samples = df2.num_samples.apply(lambda x: int(x))\n",
    "\n",
    "if True:\n",
    "    # for task in [\"sst2\", \"conll\", \"mnli\"]:\n",
    "    # sns.lineplot(data=df2[df2.negtask == task], x=\"num_samples\", y=\"gpt-4_lm_score\", hue=\"model\")\n",
    "    sns.lineplot(data=df2, x=\"num_samples\", y=\"gpt-4_lm_score\", hue=\"model\")\n",
    "\n",
    "\n",
    "plt.xscale(\"symlog\")\n",
    "plt.ylim((0, 1))\n",
    "plt.savefig(\"../outputs/plots/sym_exp1_neg.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tmp = df2[df2.num_samples == 1000].groupby([\"model\", \"num_samples\", \"negtask\"])[\"gpt-4_lm_score\"].mean().reset_index().pivot(index=\"model\", columns=\"negtask\", values=\"gpt-4_lm_score\")\n",
    "tmp[\"Base\"] = df2[df2.num_samples == 0].groupby([\"model\"])[\"gpt-4_lm_score\"].mean()\n",
    "tmp[\"Average\"] = df2[df2.num_samples == 1000].groupby([\"model\"])[\"gpt-4_lm_score\"].mean()\n",
    "print(tmp[[\"Base\", \"sst2\", \"conll\", \"mnli\", \"Average\"]].to_latex(float_format=\"%.2f\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# df2 = pd.read_csv('../outputs/results_standard.csv').sort_values(by=['model', 'num_samples'])\n",
    "df2 = pd.read_csv('../outputs/results_exp1b_standard_datasets_all.csv').sort_values(by=['model', 'num_samples'])\n",
    "# df2 = pd.read_csv('../outputs/results_exp1b_standard_datasets_2023-05-15.csv').sort_values(by=['model', 'num_samples'])\n",
    "# tmp = pd.read_csv('../outputs/results_exp1b_standard_datasets_backup.csv')\n",
    "# df2 = pd.concat([df2, tmp[tmp.model == \"xsum\"]]).fillna(0)\n",
    "\n",
    "df2[\"target_class\"] = df2.model\n",
    "# df2 = df2[df2.model != \"squadv2\"]\n",
    "cols = [\"gpt-4_lm_score\"]\n",
    "for col in cols:\n",
    "    df2[col] = df2[col]/10\n",
    "# df2.mean_score_pred = df2.mean_score_pred/10\n",
    "# df2.mean_score_ref = df2.mean_score_ref/10\n",
    "df2 = df2[df2.num_samples < 5000]\n",
    "# df2.loc[df2.model == \"conll\", \"custom_score\"] = df2[df2.model == \"conll\"].apply(lambda x: x.custom_score*0.5  + x.rouge1*0.5, axis=1)\n",
    "df2.num_samples = df2.num_samples.apply(lambda x: int(x) if x != \"only\" else 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2col = {\n",
    "    \"conll\": \"red\",\n",
    "    \"mnli\": \"blue\",\n",
    "    \"sst2\": \"green\",\n",
    "    # \"stsb\": \"black\",\n",
    "}\n",
    "\n",
    "\n",
    "model2col = {\n",
    "    \"llama\": \"red\",\n",
    "    \"falcon\": \"blue\",\n",
    "    \"bloom\": \"green\",\n",
    "    \"pythia\": \"orange\",\n",
    "}\n",
    "target_col = [\"rouge1\",\n",
    "              \"gpt-4_lm_score\",\n",
    "              # \"gpt-4_mean_score_ratio\",\n",
    "              \"exact_match\",\n",
    "              \"custom_score\"]\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(ncols=3, nrows=4, figsize=(6, 6))\n",
    "fig.tight_layout()\n",
    "\n",
    "for j, model in enumerate([\"pythia\", \"falcon\", \"bloom\", \"llama\"]):\n",
    "    for i, cat in enumerate(label2col.keys()):\n",
    "        data_df = df2[(df2.task == cat) & (df2.model == model)].sort_values(by=\"num_samples\")\n",
    "        # data_df = data_df.groupby([\"target_class\", \"num_samples\"])[target_col].mean().reset_index()\n",
    "        ref_h = data_df[(data_df.num_samples == 0)][\"gpt-4_lm_score\"].mean()\n",
    "\n",
    "        axs[j][i].axhline(y=ref_h, color='black', linestyle='dashdot', alpha=0.3)\n",
    "        sns.lineplot(x=\"num_samples\", y=\"exact_match\",\n",
    "                     data=data_df, label=cat, color=model2col[model], ax=axs[j][i])\n",
    "\n",
    "        sns.lineplot(x=\"num_samples\", y=\"custom_score\",\n",
    "                     data=data_df, linestyle='dashed', color=model2col[model], ax=axs[j][i])\n",
    "\n",
    "        sns.lineplot(x=\"num_samples\", y=\"gpt-4_lm_score\",\n",
    "                     data=data_df, linestyle='dotted', color=model2col[model], ax=axs[j][i])\n",
    "\n",
    "        axs[j][i].set_xlim(0, 1000)\n",
    "\n",
    "        axs[j][i].set_xscale('symlog')\n",
    "        axs[j][i].set(xlabel=r'$N$' if j==3 else '', ylabel=model.capitalize() if i==0 else '')\n",
    "        if j==0:\n",
    "            axs[j][i].set_title(cat)\n",
    "        axs[j][i].get_legend().remove()\n",
    "\n",
    "        def font_up(ax):\n",
    "            for item in ([ax.title, ax.xaxis.label, ax.yaxis.label]):\n",
    "                    item.set_fontsize(14)\n",
    "\n",
    "        font_up(axs[j][i])\n",
    "        font_up(axs[j][i])\n",
    "\n",
    "# if False:\n",
    "# # old llama data\n",
    "#     label2col = {\n",
    "#         \"conll\": \"red\",\n",
    "#         \"mnli\": \"blue\",\n",
    "#         \"sst2\": \"green\",\n",
    "#         \"qnli\": \"yellow\",\n",
    "#         \"squadv2\": \"orange\",\n",
    "#         # \"stsb\": \"black\",\n",
    "#         \"xsum\": \"gray\"\n",
    "#     }\n",
    "#     for i, cat in enumerate([\"squadv2\", \"qnli\", \"xsum\"]):\n",
    "#\n",
    "#         ref_h = df2_save[(df2_save.model == cat) & (df2_save.target_class == cat) & (df2_save.num_samples == 0)][\"gpt-4_lm_score\"].mean()\n",
    "#\n",
    "#         axs[4][i%3].axhline(y=ref_h, color='black', linestyle='doshdat')\n",
    "#\n",
    "#\n",
    "#         sns.lineplot(x=\"num_samples\", y=\"exact_match\",\n",
    "#                      data=df2_save[(df2_save.model == cat) & (df2_save.target_class == cat)].sort_values(by=\"num_samples\"), label=cat, color=label2col[cat], ax=axs[4][i%3])\n",
    "#\n",
    "#\n",
    "#         sns.lineplot(x=\"num_samples\", y=\"custom_score\",\n",
    "#                      data=df2_save[(df2_save.model == cat) & (df2_save.target_class == cat)].sort_values(by=\"num_samples\"), label=cat, linestyle='dashed', color=label2col[cat], ax=axs[4][i%3])\n",
    "#\n",
    "#         sns.lineplot(x=\"num_samples\", y=\"gpt-4_lm_score\",\n",
    "#                      data=df2_save[(df2_save.model == cat) & (df2_save.target_class == cat)].sort_values(by=\"num_samples\"), linestyle='dotted', color=label2col[cat], ax=axs[4][i%3])\n",
    "#\n",
    "#\n",
    "#\n",
    "#         axs[4][i%3].set_xlim(0, 1000)\n",
    "#         axs[4][i%3].set_xscale('symlog')\n",
    "#         axs[4][i%3].set(xlabel=cat.upper(), ylabel=model.capitalize() if i==0 else '')\n",
    "#         axs[4][i%3].get_legend().remove()\n",
    "\n",
    "\n",
    "from matplotlib.lines import Line2D\n",
    "line1 = Line2D([0], [0], label='Exact Match', color='k')\n",
    "line2 = Line2D([0], [0], label='Human Score', color='k', linestyle='dashed')\n",
    "line3 = Line2D([0], [0], label='GPT4 Score', color='k', linestyle='dotted')\n",
    "\n",
    "handles= [line1, line2, line3]\n",
    "axs[0][0].legend(fontsize=12, handles=handles, loc='lower left', bbox_to_anchor=(-0.1, 1.26), ncol=3, borderaxespad=0., frameon=False)\n",
    "\n",
    "    # ax2.get_legend().remove()\n",
    "\n",
    "# where some data has already been plotted to ax\n",
    "# handles, labels = ax2.get_legend_handles_labels()\n",
    "# handles.extend([line1, line2])\n",
    "\n",
    "# plot the legend\n",
    "# ax1.set(xlabel='# of training samples', ylabel='Mean Score')\n",
    "\n",
    "\n",
    "# set x-axis to log scale\n",
    "plt.xscale('symlog')\n",
    "plt.ylim((0, 1))\n",
    "plt.savefig(\"../outputs/plots/exp1b_all.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df2_save[(df2_save.model == cat) & (df2_save.target_class == cat) & (df2_save.num_samples == 0) & (df2_save.num_samples == 0)][\"gpt-4_lm_score\"].mean()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "df = pd.read_csv(\"../outputs/results_exp2_ins_diversity.csv\")\n",
    "sns.lineplot(data=df, x=\"num_bins\", y=\"custom_score\", hue=\"test_data\")\n",
    "plt.xscale(\"log\")\n",
    "plt.ylim((0, 1))\n",
    "plt.savefig(\"../outputs/plots/exp2.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"test_data\")[\"custom_score\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.boxplot(data=df, x=\"test_data\", y=\"custom_score\")\n",
    "ax.set(xlabel='Task', ylabel='Mean Score')\n",
    "plt.ylim((0, 0.9))\n",
    "plt.savefig(\"../outputs/plots/exp2_boxplot.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"custom_score\", \"num_bins\"]].corr(\"pearson\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exp3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# SST2 results (0-shot)\n",
    "PATH = \"../outputs/results_exp3_scaling.csv\"\n",
    "df = pd.read_csv(PATH) # .groupby([\"num_samples\"])[\"custom_score\"].median().reset_index()\n",
    "# Remove 0.95 quantile and 0.05 quantile for each bin\n",
    "\n",
    "# Define a function to filter\n",
    "# the upper and lower 5% of data for each category\n",
    "def filter_quantiles(group):\n",
    "    q05 = group['custom_score'].quantile(0.10)\n",
    "    q95 = group['custom_score'].quantile(0.90)\n",
    "    return group[(group['custom_score'] >= q05) & (group['custom_score'] <= q95)]\n",
    "\n",
    "# Apply the function to each group and concatenate the filtered dataframes back together\n",
    "# df = pd.concat([filter_quantiles(group) for name, group in df.groupby('num_samples')])\n",
    "\n",
    "\n",
    "sns.barplot(data=df, x=\"num_samples\", y=\"custom_score\", label=\"custom_score\")\n",
    "# sns.lineplot(data=df, x=\"num_samples\", y=\"custom_score\", label=\"custom_score\")\n",
    "# plt.xscale(\"log\")\n",
    "# sns.lineplot(data=df, x=\"num_samples\", y=\"exact_match\", label=\"exact_match\")\n",
    "plt.ylim((0.6, 0.9))\n",
    "plt.savefig(\"../outputs/plots/exp3.pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "# TODO: add another category\n",
    "# plt.figure(figsize=(10,5))\n",
    "# SST2 results (0-shot)\n",
    "PATH = \"../outputs/results_exp3_scaling.csv\"\n",
    "df = pd.read_csv(PATH) # .groupby([\"num_samples\"])[\"custom_score\"].median().reset_index()\n",
    "df[\"gpt-4_lm_score\"] /= 10\n",
    "# Remove 0.95 quantile and 0.05 quantile for each bin\n",
    "# df = df[df.task == \"sst2\"]\n",
    "# Define a function to filter\n",
    "# the upper and lower 5% of data for each category\n",
    "def filter_quantiles(group):\n",
    "    q05 = group['custom_score'].quantile(0.10)\n",
    "    q95 = group['custom_score'].quantile(0.90)\n",
    "    return group[(group['custom_score'] >= q05) & (group['custom_score'] <= q95)]\n",
    "\n",
    "# Apply the function to each group and concatenate the filtered dataframes back together\n",
    "# df = pd.concat([filter_quantiles(group) for name, group in df.groupby('num_samples')])\n",
    "df[\"score\"] = df.apply(lambda x: x[\"custom_score\"] if x[\"custom_score\"] != -1 else x[\"gpt-4_lm_score\"], axis=1)\n",
    "df = df[~df.num_samples.isin([1000, 5000, 10000])] # .groupby(\"num_samples\").tail(5).reset_index()\n",
    "df = df[~df.task.isin([\"xsum\"])]#.groupby([\"task\", \"num_samples\"]).tail(5).reset_index()\n",
    "\n",
    "ax = sns.boxplot(data=df, x=\"num_samples\", y=\"score\", hue=\"task\", width=0.5)\n",
    "ax.set(xlabel='# of training samples', ylabel='Mean Score')\n",
    "# sns.lineplot(data=df, x=\"num_samples\", y=\"custom_score\", label=\"custom_score\")\n",
    "# plt.xscale(\"log\")\n",
    "# sns.lineplot(data=df, x=\"num_samples\", y=\"exact_match\", label=\"exact_match\")\n",
    "plt.ylim((0.6, 1))\n",
    "plt.savefig(\"../outputs/plots/exp3.pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[~df.task.isin([\"xsum\"])].groupby([\"task\", \"num_samples\"]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"num_samples\").count()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from scipy.ndimage.filters import gaussian_filter1d\n",
    "\n",
    "# SST2 results (0-shot)\n",
    "PATH = \"../outputs/results_exp3_scaling.csv\"\n",
    "df = pd.read_csv(PATH)\n",
    "\n",
    "# plt.plot(df.num_samples, gaussian_filter1d(df.groupby([\"num_samples\"])[\"custom_score\"].mean().reset_index()[\"custom_score\"], sigma=0.5), label=\"custom_score\")\n",
    "sns.lineplot(data=df, x=\"num_samples\", y=\"custom_score\", label=\"custom_score\")\n",
    "plt.xscale(\"symlog\")\n",
    "plt.xlim((100, 10000))\n",
    "# sns.lineplot(data=df, x=\"num_samples\", y=\"exact_match\", label=\"exact_match\")\n",
    "plt.ylim((0.5, 0.9))\n",
    "# plt.savefig(\"../outputs/plots/exp3.pdf\")import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# SST2 results (0-shot)\n",
    "PATH = \"../outputs/results_exp3_scaling.csv\"\n",
    "df = pd.read_csv(PATH) # .groupby([\"num_samples\"])[\"custom_score\"].median().reset_index()\n",
    "# Remove 0.95 quantile and 0.05 quantile for each bin\n",
    "\n",
    "# Define a function to filter\n",
    "# the upper and lower 5% of data for each category\n",
    "def filter_quantiles(group):\n",
    "    q05 = group['custom_score'].quantile(0.10)\n",
    "    q95 = group['custom_score'].quantile(0.90)\n",
    "    return group[(group['custom_score'] >= q05) & (group['custom_score'] <= q95)]\n",
    "\n",
    "# Apply the function to each group and concatenate the filtered dataframes back together\n",
    "# df = pd.concat([filter_quantiles(group) for name, group in df.groupby('num_samples')])\n",
    "\n",
    "\n",
    "sns.barplot(data=df, x=\"num_samples\", y=\"custom_score\", label=\"custom_score\")\n",
    "# sns.lineplot(data=df, x=\"num_samples\", y=\"custom_score\", label=\"custom_score\")\n",
    "# plt.xscale(\"log\")\n",
    "# sns.lineplot(data=df, x=\"num_samples\", y=\"exact_match\", label=\"exact_match\")\n",
    "plt.ylim((0.6, 0.9))\n",
    "plt.savefig(\"../outputs/plots/exp3.pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby([\"num_samples\"])[\"custom_score\"].count().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.num_samples == 1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exp 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def smooth(scalars: List[float], weight: float) -> List[float]:  # Weight between 0 and 1\n",
    "    last = scalars[0]  # First value in the plot (first timestep)\n",
    "    smoothed = list()\n",
    "    for point in scalars:\n",
    "        smoothed_val = last * weight + (1 - weight) * point  # Calculate smoothed value\n",
    "        smoothed.append(smoothed_val)                        # Save it\n",
    "        last = smoothed_val                                  # Anchor the last smoothed value\n",
    "\n",
    "    return smoothed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "PATH = \"../outputs/results_exp4_ift_noift.csv\"\n",
    "df = pd.read_csv(PATH)\n",
    "# df[\"gpt-4_lm_score\"] = df[\"gpt-4_lm_score\"]/10\n",
    "df = df[df.task == \"sst2\"]\n",
    "df[\"gpt-4_lm_score\"] = df[\"gpt-4_lm_score\"]/10\n",
    "df.model = df.model.apply(lambda x: x[1:])\n",
    "# sns.lineplot(data=df[df.ift == \"ift\"], x=\"num_samples\", y=\"custom_score\",  hue=\"model\")\n",
    "# sns.lineplot(data=df[df.ift == \"no_ift\"], x=\"num_samples\", y=\"custom_score\",  hue=\"model\", linestyle=\"dashed\")\n",
    "sns.lineplot(data=df[df.ift == \"ift\"], x=\"num_samples\", y=\"gpt-4_lm_score\",  hue=\"model\")\n",
    "sns.lineplot(data=df[df.ift == \"no_ift\"], x=\"num_samples\", y=\"gpt-4_lm_score\",  hue=\"model\", linestyle=\"dashed\")\n",
    "plt.xscale(\"symlog\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "label2col = {\n",
    "\"llama\": \"red\",\n",
    "\"falcon\": \"blue\",\n",
    "\"bloom\": \"green\",\n",
    "\"pythia\": \"orange\",\n",
    "\"opt\": \"yellow\",\n",
    "# \"stsb\": \"black\",\n",
    "\"xsum\": \"gray\"\n",
    "}\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(ncols=2, nrows=2, figsize=(4.5, 3))\n",
    "fig.tight_layout()\n",
    "\n",
    "for i, cat in enumerate(df.model.unique()):\n",
    "\n",
    "    ref_h = df[(df.model == cat) & (df.ift == \"ift\") &  (df.num_samples == 0)][\"gpt-4_lm_score\"].mean()\n",
    "\n",
    "    axs[i//2][i%2].axhline(y=ref_h, color='black', linestyle='dashdot', alpha=0.3)\n",
    "\n",
    "    sns.lineplot(x=\"num_samples\", y=\"gpt-4_lm_score\",\n",
    "                 data=df[(df.model == cat) & (df.ift == \"ift\")].sort_values(by=\"num_samples\"), label=cat, color=label2col[cat], linestyle='solid', errorbar=None, ax=axs[i//2][i%2])\n",
    "\n",
    "\n",
    "    sns.lineplot(x=\"num_samples\", y=\"gpt-4_lm_score\",\n",
    "                 data=df[(df.model == cat) & (df.ift == \"iftr\")].sort_values(by=\"num_samples\"), label=cat, color=label2col[cat], linestyle='dotted', errorbar=None, ax=axs[i//2][i%2])\n",
    "\n",
    "    sns.lineplot(x=\"num_samples\", y=\"gpt-4_lm_score\",\n",
    "                 data=df[(df.model == cat) & (df.ift == \"no_ift\")].sort_values(by=\"num_samples\"), label=cat, color=label2col[cat], linestyle='dashed', errorbar=None, ax=axs[i//2][i%2])\n",
    "    axs[i//2][i%2].set_xscale('symlog')\n",
    "    # axs[i//2][i%2].set_xlim((0, 100))\n",
    "    axs[i//2][i%2].set(xlabel=cat.capitalize(), ylabel='GPT-4 Score' if i%2 == 0 else '')\n",
    "    axs[i//2][i%2].get_legend().remove()\n",
    "\n",
    "line1 = Line2D([0], [0], label='Alpaca', color='k')\n",
    "line2 = Line2D([0], [0], label='Base', color='k', linestyle='dashed')\n",
    "line3 = Line2D([0], [0], label='Shelf', color='k', linestyle='dotted')\n",
    "\n",
    "handles= [line1, line2, line3]\n",
    "axs[0][0].legend(handles=handles, loc='lower right', bbox_to_anchor=(1.0, 0.0), frameon=False)\n",
    "# sns.lineplot(data=df, x=\"num_samples\", y=\"gpt-4_lm_score\",  hue=\"model\", linestyle=\"dashed\")\n",
    "# sns.lineplot(data=df, x=\"num_samples\", y=\"exact_match\", label=\"exact_match\")\n",
    "\n",
    "plt.savefig(\"../outputs/plots/exp4.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=2, nrows=2, figsize=(4.5, 3))\n",
    "fig.tight_layout()\n",
    "\n",
    "smoothing_factor = 0.1\n",
    "for i, cat in enumerate(df.model.unique()):\n",
    "\n",
    "    tdf = df[(df.model == cat) & (df.ift == \"ift\")].groupby(\"num_samples\")[\"gpt-4_lm_score\"].median().reset_index().sort_values(by=\"num_samples\")\n",
    "    tdf[\"gpt-4_lm_score\"] = smooth(tdf[\"gpt-4_lm_score\"],smoothing_factor)\n",
    "\n",
    "    sns.lineplot(x=\"num_samples\", y=\"gpt-4_lm_score\",\n",
    "                 data=tdf, label=cat, color=label2col[cat], linestyle='solid', errorbar=None, ax=axs[i//2][i%2])\n",
    "\n",
    "    tdf = df[(df.model == cat) & (df.ift == \"iftr\")].groupby(\"num_samples\")[\"gpt-4_lm_score\"].median().reset_index().sort_values(by=\"num_samples\")\n",
    "    tdf[\"gpt-4_lm_score\"] = smooth(tdf[\"gpt-4_lm_score\"], smoothing_factor)\n",
    "\n",
    "    sns.lineplot(x=\"num_samples\", y=\"gpt-4_lm_score\",\n",
    "                 data=tdf, label=cat, color=label2col[cat], linestyle='dashdot', errorbar=None, ax=axs[i//2][i%2])\n",
    "\n",
    "    tdf = df[(df.model == cat) & (df.ift == \"no_ift\")].groupby(\"num_samples\")[\"gpt-4_lm_score\"].median().reset_index().sort_values(by=\"num_samples\")\n",
    "    tdf[\"gpt-4_lm_score\"] = smooth(tdf[\"gpt-4_lm_score\"], smoothing_factor)\n",
    "\n",
    "    sns.lineplot(x=\"num_samples\", y=\"gpt-4_lm_score\",\n",
    "                 data=tdf, label=cat, color=label2col[cat], linestyle='dashed', errorbar=None, ax=axs[i//2][i%2])\n",
    "\n",
    "\n",
    "    axs[i//2][i%2].set_xscale('symlog')\n",
    "    # axs[i//2][i%2].set_ylim(0, 1)\n",
    "    # axs[i//2][i%2].set_xlim((0, 100))\n",
    "    axs[i//2][i%2].set(xlabel=cat.capitalize(), ylabel='')\n",
    "    axs[i//2][i%2].get_legend().remove()\n",
    "\n",
    "\n",
    "line1 = Line2D([0], [0], label='Alpaca', color='k')\n",
    "line2 = Line2D([0], [0], label='Base', color='k', linestyle='dashed')\n",
    "line3 = Line2D([0], [0], label='Shelf', color='k', linestyle='dotted')\n",
    "\n",
    "handles= [line1, line2, line3]\n",
    "axs[0][0].legend(handles=handles, loc='lower right', bbox_to_anchor=(1.0, 0.0), frameon=False)\n",
    "\n",
    "plt.savefig(\"../outputs/plots/exp4_all_old.pdf\", bbox_inches='tight')\n",
    "\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(ncols=2, nrows=2, figsize=(4.5, 3))\n",
    "fig.tight_layout()\n",
    "\n",
    "smoothing_factor = 0.1\n",
    "for i, cat in enumerate(df.model.unique()):\n",
    "\n",
    "    tdf = df[(df.model == cat) & (df.ift == \"ift\")].groupby(\"num_samples\")[\"gpt-4_lm_score\"].median().reset_index().sort_values(by=\"num_samples\")\n",
    "    tdf[\"gpt-4_lm_score\"] = smooth(tdf[\"gpt-4_lm_score\"],smoothing_factor)\n",
    "\n",
    "    ref_h = tdf[tdf[\"num_samples\"] == 0][\"gpt-4_lm_score\"].values[0]\n",
    "    axs[i//2][i%2].axhline(y=ref_h, color='black', linestyle='dashdot', alpha=0.3)\n",
    "\n",
    "    sns.lineplot(x=\"num_samples\", y=\"gpt-4_lm_score\",\n",
    "                 data=tdf, label=cat, color=label2col[cat], linestyle='solid', errorbar=None, ax=axs[i//2][i%2])\n",
    "\n",
    "    tdf = df[(df.model == cat) & (df.ift == \"no_ift\")].groupby(\"num_samples\")[\"gpt-4_lm_score\"].median().reset_index().sort_values(by=\"num_samples\")\n",
    "    tdf[\"gpt-4_lm_score\"] = smooth(tdf[\"gpt-4_lm_score\"], smoothing_factor)\n",
    "\n",
    "    sns.lineplot(x=\"num_samples\", y=\"gpt-4_lm_score\",\n",
    "                 data=tdf, label=cat, color=label2col[cat], linestyle='dashed', errorbar=None, ax=axs[i//2][i%2])\n",
    "\n",
    "\n",
    "    axs[i//2][i%2].set_xscale('symlog')\n",
    "    # axs[i//2][i%2].set_ylim(0, 1)\n",
    "    # axs[i//2][i%2].set_xlim((0, 100))\n",
    "    # .set(xlabel=cat.capitalize(), ylabel='GPT-4 Score' if i%2 == 0 else '')\n",
    "    axs[i//2][i%2].get_legend().remove()\n",
    "    axs[i//2][i%2].set(xlabel=r'$N$' if i//2==1 else '', ylabel=\"\" if i==0 else '')\n",
    "\n",
    "    axs[i//2][i%2].set_title(cat.capitalize())\n",
    "\n",
    "    def font_up(ax):\n",
    "            for item in ([ax.title, ax.xaxis.label, ax.yaxis.label]):\n",
    "                    item.set_fontsize(12)\n",
    "\n",
    "    font_up(axs[i//2][i%2])\n",
    "\n",
    "o = fig.text(-0.01, 0.5, 'GPT4 Score', va='center', rotation='vertical')\n",
    "o.set_fontsize(12)\n",
    "line1 = Line2D([0], [0], label='Alpaca', color='k')\n",
    "line2 = Line2D([0], [0], label='Base Model', color='k', linestyle='dashed')\n",
    "\n",
    "handles= [line1, line2]\n",
    "# axs[0][0].legend(handles=handles, loc='lower right', bbox_to_anchor=(1.0, 0.0), frameon=False)\n",
    "axs[0][0].legend(fontsize=12, handles=handles, loc='lower left', bbox_to_anchor=(0.25, 1.13), ncol=2, borderaxespad=0., frameon=False)\n",
    "\n",
    "plt.savefig(\"../outputs/plots/exp4_all.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf[tdf[\"num_samples\"] == 0][\"gpt-4_lm_score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "PATH = \"../outputs/results_exp4_alpaca_llama.csv\"\n",
    "df = pd.read_csv(PATH)\n",
    "# df[\"gpt-4_lm_score\"] = df[\"gpt-4_lm_score\"]/10\n",
    "df = df[df.task == \"sst2\"]\n",
    "df.model = df.model.apply(lambda x: x[1:])\n",
    "sns.lineplot(data=df, x=\"num_samples\", y=\"custom_score\",  hue=\"model\")\n",
    "# sns.lineplot(data=df, x=\"num_samples\", y=\"gpt-4_lm_score\",  hue=\"model\", linestyle=\"dashed\")\n",
    "# sns.lineplot(data=df, x=\"num_samples\", y=\"exact_match\", label=\"exact_match\")\n",
    "\n",
    "plt.xscale(\"log\")\n",
    "plt.savefig(\"../outputs/plots/exp4.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "PATH = \"../outputs/results_exp4_alpaca_llama.csv\"\n",
    "df = pd.read_csv(PATH)\n",
    "# df[\"gpt-4_lm_score\"] = df[\"gpt-4_lm_score\"]/10\n",
    "df = df[df.task == \"squadv2\"]\n",
    "df.model = df.model.apply(lambda x: x[1:])\n",
    "sns.lineplot(data=df, x=\"num_samples\", y=\"custom_score\",  hue=\"model\")\n",
    "# sns.lineplot(data=df, x=\"num_samples\", y=\"gpt-4_lm_score\",  hue=\"model\", linestyle=\"dashed\")\n",
    "# sns.lineplot(data=df, x=\"num_samples\", y=\"exact_match\", label=\"exact_match\")\n",
    "\n",
    "plt.xscale(\"log\")\n",
    "plt.savefig(\"../outputs/plots/exp4.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"../outputs/results_exp4_alpaca_llama_backup.csv\"\n",
    "df = pd.read_csv(PATH)\n",
    "df[\"gpt-4_lm_score\"] = df[\"gpt-4_lm_score\"]/10\n",
    "df = df[df.task == \"xsum\"]\n",
    "df.model = df.model.apply(lambda x: x[1:])\n",
    "sns.lineplot(data=df, x=\"num_samples\", y=\"rouge1\",  hue=\"model\")\n",
    "sns.lineplot(data=df, x=\"num_samples\", y=\"gpt-4_lm_score\",  hue=\"model\", linestyle=\"dashed\")\n",
    "# sns.lineplot(data=df, x=\"num_samples\", y=\"exact_match\", label=\"exact_match\")\n",
    "\n",
    "plt.xscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "PATH = \"../outputs/results_exp4_alpaca_llama.csv\"\n",
    "df = pd.read_csv(PATH)\n",
    "df.model = df.model.apply(lambda x: x[1:].capitalize())\n",
    "ax = sns.boxplot(data=df, x=\"num_samples\", y=\"custom_score\",  hue=\"model\")\n",
    "# plt.legend(handles=handles, loc='upper left')\n",
    "ax.set(xlabel='# of training samples', ylabel='Mean Score')\n",
    "# sns.lineplot(data=df, x=\"num_samples\", y=\"exact_match\", label=\"exact_match\")\n",
    "plt.savefig(\"../outputs/plots/exp4.pdf\")\n",
    "\n",
    "# plt.xscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby([\"num_samples\", \"model\"]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "PATH = \"../outputs/results_exp4_alpaca_llama.csv\"\n",
    "df = pd.read_csv(PATH)\n",
    "df.model = df.model.apply(lambda x: x[1:].capitalize())\n",
    "fig, axs = plt.subplots(ncols=3, figsize=(15, 3))\n",
    "fig.tight_layout()\n",
    "\n",
    "for i, cat in enumerate([\"sst2\", \"xsum\", \"squadv2\"]):\n",
    "    df2 = df[df.task == cat]\n",
    "    ax = sns.boxplot(data=df2, x=\"num_samples\", y=\"custom_score\",  hue=\"model\", ax=axs[i], width=0.6)\n",
    "    axs[i].set(xlabel=cat, ylabel='')\n",
    "    axs[i].get_legend().remove()\n",
    "\n",
    "\n",
    "# plt.legend(handles=handles, loc='upper left')\n",
    "# ax.set(xlabel='# of training samples', ylabel='Mean Score')\n",
    "# sns.lineplot(data=df, x=\"num_samples\", y=\"exact_match\", label=\"exact_match\")\n",
    "plt.savefig(\"../outputs/plots/exp4.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exp 0-a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "PATH = \"../outputs/results_exp0_sum.csv\"\n",
    "df = pd.read_csv(PATH)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = [\"rouge1\",\n",
    "              \"gpt-4_lm_score\",\n",
    "              # \"gpt-4_mean_score_ratio\",\n",
    "              # \"gpt-3.5-turbo_lm_score\",\n",
    "              \"bertscore\",\n",
    "              \"sbertscore\",\n",
    "              # \"reward_model_score\",\n",
    "              # \"softmaxed_reward_model_score\",\n",
    "              \"litepyramid_recall\"]\n",
    "\n",
    "clean_target_col = [\"ROUGE-1\",\n",
    "              \"GPT-4\",\n",
    "              # \"gpt-4_mean_score_ratio\",\n",
    "              # \"GPT-3.5\",\n",
    "              \"BERTScore\",\n",
    "              \"SBert\",\n",
    "              # \"RM\",\n",
    "              # \"Soft RM\",\n",
    "              \"Human\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df[target_col].corr(method=\"spearman\")\n",
    "matrix = np.triu(corr, k=1)\n",
    "# plot the heatmap\n",
    "assert (corr.columns == corr.index).all()\n",
    "print(corr.columns, clean_target_col )\n",
    "corr.index.name = \"\"\n",
    "\n",
    "# display(corr)\n",
    "# plot the heatmap\n",
    "sns.heatmap(corr,\n",
    "        xticklabels=clean_target_col,\n",
    "        yticklabels=clean_target_col,\n",
    "            annot=True, mask=matrix, vmin=0, vmax=1,\n",
    "            cmap=\"coolwarm\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 15\n",
    "df.iloc[k].model_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[k][target_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(df, x='litepyramid_recall', y='gpt-4_lm_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"model\").describe()[target_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = df[df.litepyramid_recall > 0.8].sample(1).iloc[0]\n",
    "display(sample[target_col])\n",
    "print(sample[\"model_summary\"])\n",
    "print(sample[\"ref_summary\"])\n",
    "print(\"-----------\")\n",
    "print(sample[\"source\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exp 0B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = [\"rouge1\",\n",
    "              \"rougeL\",\n",
    "              \"gpt-4_lm_score\",\n",
    "              \"gpt-4_mean_score_ratio\",\n",
    "              \"gpt-3.5-turbo_lm_score\",\n",
    "              \"gpt-3.5-turbo_mean_score_ratio\",\n",
    "              \"bertscore\",\n",
    "              \"sbertscore\",\n",
    "              \"reward_model_score\",\n",
    "              \"softmaxed_reward_model_score\",\n",
    "              \"human_score\"]\n",
    "\n",
    "clean_target_col = [\"ROUGE-1\", \"ROUGE-L\",\n",
    "              \"GPT-4\",\n",
    "              \"GPT-4 Ratio\",\n",
    "              \"GPT-3.5\",\n",
    "              \"GPT-3.5 Ratio\",\n",
    "              \"BERTScore\",\n",
    "              \"SBert\",\n",
    "              \"RM\",\n",
    "              \"Soft RM\",\n",
    "              \"Human\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "PATH = \"../outputs/results_exp0b_sum.csv\"\n",
    "df = pd.read_csv(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_counts = df.groupby(['human_score', 'gpt-4_lm_score']).size().reset_index(name='count')\n",
    "plt.figure()\n",
    "sns.scatterplot(df_counts, x='human_score', y='gpt-4_lm_score', size=\"count\", sizes=(50, 200))\n",
    "\n",
    "plt.figure()\n",
    "sns.kdeplot(data=df_counts,  x='human_score', y='gpt-4_lm_score', fill=True, levels=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df[target_col].corr(method=\"spearman\")\n",
    "matrix = np.triu(corr, k=1)\n",
    "# plot the heatmap\n",
    "assert (corr.columns == corr.index).all()\n",
    "print(corr.columns, clean_target_col )\n",
    "corr.index.name = \"\"\n",
    "\n",
    "# display(corr)\n",
    "# plot the heatmap\n",
    "sns.heatmap(corr,\n",
    "        xticklabels=clean_target_col,\n",
    "        yticklabels=clean_target_col,\n",
    "            annot=True, mask=matrix, vmin=0, vmax=1,\n",
    "            cmap=\"coolwarm\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "policies = list(df.policy.unique())\n",
    "# df[\"random_group\"] = df.apply(lambda x: f\"{x.policy}_{x.id[-1]}\", axis=1)\n",
    "l = []\n",
    "for i in range(10000):\n",
    "    l.append(df.sample(10, replace=False)[target_col].corr(\"spearman\"))\n",
    "\n",
    "corr = pd.concat(l)\n",
    "corr.index.name = \"scorer\"\n",
    "corr = corr.groupby(\"scorer\").mean().reindex(target_col)[target_col]\n",
    "\n",
    "matrix = np.triu(corr, k=1)\n",
    "# plot the heatmap\n",
    "assert (corr.columns == corr.index).all()\n",
    "print(corr.columns, clean_target_col)\n",
    "corr.index.name = \"\"\n",
    "\n",
    "# display(corr)\n",
    "# plot the heatmap\n",
    "sns.heatmap(corr,\n",
    "            xticklabels=clean_target_col,\n",
    "            yticklabels=clean_target_col,\n",
    "            annot=True, mask=matrix, vmin=0, vmax=1,\n",
    "            cmap=\"coolwarm\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s0 = pd.DataFrame()\n",
    "s0[\"s1\"] = corr[[\"human_score\"]]\n",
    "s0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "policies = list(df.policy.unique())\n",
    "# df[\"random_group\"] = df.apply(lambda x: f\"{x.policy}_{x.id[-1]}\", axis=1)\n",
    "l = []\n",
    "for i in range(1000):\n",
    "    pol = random.choice(policies)\n",
    "    l.append(df[df.policy == pol].sample(10, replace=True)[target_col].corr(\"spearman\"))\n",
    "\n",
    "corr = pd.concat(l)\n",
    "corr.index.name = \"scorer\"\n",
    "corr = corr.groupby(\"scorer\").mean().reindex(target_col)[target_col]\n",
    "\n",
    "matrix = np.triu(corr, k=1)\n",
    "# plot the heatmap\n",
    "assert (corr.columns == corr.index).all()\n",
    "print(corr.columns, clean_target_col)\n",
    "corr.index.name = \"\"\n",
    "\n",
    "# display(corr)\n",
    "# plot the heatmap\n",
    "sns.heatmap(corr,\n",
    "            xticklabels=clean_target_col,\n",
    "            yticklabels=clean_target_col,\n",
    "            annot=True, mask=matrix, vmin=0, vmax=1,\n",
    "            cmap=\"coolwarm\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s0[\"s3\"] = corr[[\"human_score\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df.groupby(\"policy\")[target_col].corr(method=\"spearman\").reset_index().groupby(\"level_1\")[target_col].mean().reindex(target_col)[target_col]\n",
    "matrix = np.triu(corr, k=1)\n",
    "# plot the heatmap\n",
    "assert (corr.columns == corr.index).all()\n",
    "print(corr.columns, clean_target_col)\n",
    "corr.index.name = \"\"\n",
    "\n",
    "# display(corr)\n",
    "# plot the heatmap\n",
    "sns.heatmap(corr,\n",
    "        xticklabels=clean_target_col,\n",
    "        yticklabels=clean_target_col,\n",
    "            annot=True, mask=matrix, vmin=0, vmax=1,\n",
    "            cmap=\"coolwarm\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df.groupby(\"id\")[target_col].mean().corr(method=\"spearman\")\n",
    "matrix = np.triu(corr, k=1)\n",
    "# plot the heatmap\n",
    "assert (corr.columns == corr.index).all()\n",
    "print(corr.columns, clean_target_col )\n",
    "corr.index.name = \"\"\n",
    "\n",
    "# display(corr)\n",
    "# plot the heatmap\n",
    "sns.heatmap(corr,\n",
    "        xticklabels=clean_target_col,\n",
    "        yticklabels=clean_target_col,\n",
    "            annot=True, mask=matrix, vmin=0, vmax=1,\n",
    "            cmap=\"coolwarm\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df.groupby(\"policy\")[target_col].corr(method=\"spearman\").reset_index().groupby(\"level_1\")[target_col].mean().reindex(target_col)[target_col]\n",
    "matrix = np.triu(corr, k=1)\n",
    "# plot the heatmap\n",
    "assert (corr.columns == corr.index).all()\n",
    "print(corr.columns, clean_target_col )\n",
    "corr.index.name = \"\"\n",
    "\n",
    "# display(corr)\n",
    "# plot the heatmap\n",
    "sns.heatmap(corr,\n",
    "            xticklabels=clean_target_col,\n",
    "            yticklabels=clean_target_col,\n",
    "            annot=True, mask=matrix, vmin=0, vmax=1,\n",
    "            cmap=\"coolwarm\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df.groupby(\"id\")[target_col].corr(method=\"spearman\").reset_index().groupby(\"level_1\")[target_col].mean().reindex(target_col)[target_col]\n",
    "matrix = np.triu(corr, k=1)\n",
    "# plot the heatmap\n",
    "assert (corr.columns == corr.index).all()\n",
    "print(corr.columns, clean_target_col )\n",
    "corr.index.name = \"\"\n",
    "\n",
    "# display(corr)\n",
    "# plot the heatmap\n",
    "sns.heatmap(corr,\n",
    "        xticklabels=clean_target_col,\n",
    "        yticklabels=clean_target_col,\n",
    "            annot=True, mask=matrix, vmin=0, vmax=1,\n",
    "            cmap=\"coolwarm\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s0[\"s2\"] = corr[[ \"human_score\"]]\n",
    "s0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# df[\"random_group\"] = df.apply(lambda x: f\"{x.policy}_{x.id[-1]}\", axis=1)\n",
    "l = []\n",
    "for i in range(100):\n",
    "    df[\"random_group\"] = df.apply(lambda x: f\"{x.policy}_{random.randint(1, 100//10)}\", axis=1)\n",
    "\n",
    "    # corr = pd.concat((corr, df.groupby(\"random_group\")[target_col].mean()))\n",
    "    # print(len(corr))\n",
    "    l.append(df.groupby(\"random_group\")[target_col].mean().corr(method=\"spearman\"))\n",
    "\n",
    "corr = pd.concat(l)\n",
    "corr.index.name = \"scorer\"\n",
    "corr = corr.groupby(\"scorer\").mean().reindex(target_col)[target_col]\n",
    "corr = corr.corr(method=\"spearman\")\n",
    "matrix = np.triu(corr, k=1)\n",
    "# plot the heatmap\n",
    "assert (corr.columns == corr.index).all()\n",
    "print(corr.columns, clean_target_col )\n",
    "corr.index.name = \"\"\n",
    "\n",
    "# display(corr)\n",
    "# plot the heatmap\n",
    "sns.heatmap(corr,\n",
    "        xticklabels=clean_target_col,\n",
    "        yticklabels=clean_target_col,\n",
    "            annot=True, mask=matrix, vmin=0, vmax=1,\n",
    "            cmap=\"coolwarm\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "policies = list(df.policy.unique())\n",
    "# df[\"random_group\"] = df.apply(lambda x: f\"{x.policy}_{x.id[-1]}\", axis=1)\n",
    "corr = []\n",
    "for i in range(1000):\n",
    "    # df[\"random_group\"] = df.apply(lambda x: f\"{x.policy}_{random.randint(1, 100//10)}\", axis=1)\n",
    "    # df[\"random_group\"] = df.apply(lambda x: f\"_{x.id[-2:]}\", axis=1)\n",
    "    # corr.append(df[df.policy == policies[random.randint(0, len(policies) - 1)]].sample(replace=True, n=20)[target_col].mean())\n",
    "    corr.append(df.sample(replace=True, n=10)[target_col].mean())\n",
    "corr = pd.DataFrame(corr)\n",
    "corr = corr.corr(method=\"spearman\")\n",
    "matrix = np.triu(corr, k=1)\n",
    "# plot the heatmap\n",
    "assert (corr.columns == corr.index).all()\n",
    "print(corr.columns, clean_target_col )\n",
    "corr.index.name = \"\"\n",
    "\n",
    "# display(corr)\n",
    "# plot the heatmap\n",
    "sns.heatmap(corr,\n",
    "        xticklabels=clean_target_col,\n",
    "        yticklabels=clean_target_col,\n",
    "            annot=True, mask=matrix, vmin=0, vmax=1,\n",
    "            cmap=\"coolwarm\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"random_group\"] = df.apply(lambda x: f\"_{x.id[-2:]}\", axis=1)\n",
    "corr = df.groupby(\"policy\")[target_col].mean().corr(method=\"spearman\")\n",
    "matrix = np.triu(corr, k=1)\n",
    "# plot the heatmap\n",
    "assert (corr.columns == corr.index).all()\n",
    "print(corr.columns, clean_target_col )\n",
    "corr.index.name = \"\"\n",
    "\n",
    "# display(corr)\n",
    "# plot the heatmap\n",
    "sns.heatmap(corr,\n",
    "        xticklabels=clean_target_col,\n",
    "        yticklabels=clean_target_col,\n",
    "            annot=True, mask=matrix, vmin=0, vmax=1,\n",
    "            cmap=\"coolwarm\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "policies = list(df.policy.unique())\n",
    "# df[\"random_group\"] = df.apply(lambda x: f\"{x.policy}_{x.id[-1]}\", axis=1)\n",
    "l = []\n",
    "for i in range(100):\n",
    "    corr = []\n",
    "    sampled_docs = random.choices(df.id.unique(), k=100)\n",
    "    tmp_df = pd.DataFrame()\n",
    "    for doc in sampled_docs:\n",
    "        tmp_df = pd.concat((tmp_df, df[df.id.isin([doc])]))\n",
    "    for j in range(4):\n",
    "        corr.append(tmp_df[tmp_df.policy == policies[j]][target_col].mean())\n",
    "    l.append(pd.DataFrame(corr).corr(method=\"spearman\"))\n",
    "corr = pd.concat(l)\n",
    "corr.index.name = \"scorer\"\n",
    "corr = corr.groupby(\"scorer\").mean().reindex(target_col)[target_col]\n",
    "\n",
    "matrix = np.triu(corr, k=1)\n",
    "# plot the heatmap\n",
    "assert (corr.columns == corr.index).all()\n",
    "print(corr.columns, clean_target_col )\n",
    "corr.index.name = \"\"\n",
    "\n",
    "# display(corr)\n",
    "# plot the heatmap\n",
    "sns.heatmap(corr,\n",
    "        xticklabels=clean_target_col,\n",
    "        yticklabels=clean_target_col,\n",
    "            annot=True, mask=matrix, vmin=0, vmax=1,\n",
    "            cmap=\"coolwarm\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "policies = list(df.policy.unique())\n",
    "# df[\"random_group\"] = df.apply(lambda x: f\"{x.policy}_{x.id[-1]}\", axis=1)\n",
    "l = [[], [], [], []]\n",
    "for i in range(1000):\n",
    "    corr = []\n",
    "    sampled_docs = random.choices(df.id.unique(), k=100)\n",
    "    tmp_df = pd.DataFrame()\n",
    "    for doc in sampled_docs:\n",
    "        tmp_df = pd.concat((tmp_df, df[df.id.isin([doc])]))\n",
    "    for j in range(4):\n",
    "        l[j].append((tmp_df[tmp_df.policy == policies[j]][target_col].mean()))\n",
    "\n",
    "o = [pd.DataFrame(l[j]).corr(method=\"spearman\") for j in range(4)]\n",
    "corr = pd.concat(o)\n",
    "corr.index.name = \"scorer\"\n",
    "corr = corr.groupby(\"scorer\").mean().reindex(target_col)[target_col]\n",
    "\n",
    "matrix = np.triu(corr, k=1)\n",
    "# plot the heatmap\n",
    "assert (corr.columns == corr.index).all()\n",
    "print(corr.columns, clean_target_col )\n",
    "corr.index.name = \"\"\n",
    "\n",
    "# display(corr)\n",
    "# plot the heatmap\n",
    "sns.heatmap(corr,\n",
    "        xticklabels=clean_target_col,\n",
    "        yticklabels=clean_target_col,\n",
    "            annot=True, mask=matrix, vmin=0, vmax=1,\n",
    "            cmap=\"coolwarm\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "df = load_dataset(\"openai/summarize_from_feedback\", \"axis\")[\"test\"].to_pandas()\n",
    "\n",
    "# For quicker results\n",
    "# For quicker results\n",
    "df[\"human_score\"] = df[\"summary\"].apply(lambda x: x[\"axes\"][\"overall\"])\n",
    "df[\"source\"] = df[\"info\"].apply(lambda x: x[\"article\"])\n",
    "df[\"title\"] = df[\"info\"].apply(lambda x: x[\"title\"])\n",
    "df[\"id\"] = df[\"info\"].apply(lambda x: x[\"id\"])\n",
    "df[\"model_summary\"] = df[\"summary\"].apply(lambda x: x[\"text\"])\n",
    "df[\"policy\"] = df[\"summary\"].apply(lambda x: x[\"policy\"])\n",
    "df = df.groupby([\"id\", \"policy\", \"worker\"]).first().reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby([\"id\", \"policy\"]).count().sort_values(by=\"info\", ascending=False).head(368)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "workers = df.worker.unique()\n",
    "l = []\n",
    "for w1, w2 in product(workers, workers):\n",
    "    if w1 != w2:\n",
    "        tmp_df = df[df.worker.isin([w1, w2])].copy()\n",
    "        tmp_df[\"polid\"] = tmp_df.policy + tmp_df.id\n",
    "        a = tmp_df.groupby(([\"id\", \"policy\"])).count().reset_index()\n",
    "        a = a[a[\"info\"]>1]\n",
    "        a[\"polid\"] = a.policy + a.id\n",
    "        b = tmp_df[tmp_df.apply(lambda x: (x.policy + x.id) in list(a.polid), axis=1)]\n",
    "        b = b.groupby([\"id\", \"policy\", \"worker\"]).first().reset_index()\n",
    "        b1 = b[b.worker == w1][[\"human_score\", \"polid\"]]\n",
    "        b2 = b[b.worker == w2][[\"human_score\", \"polid\"]]\n",
    "        b = pd.merge(b1, b2, on=\"polid\", how=\"inner\").drop(\"polid\", axis=1)\n",
    "        # print(len(b))\n",
    "        if len(b) >= 10:\n",
    "            # display(b)\n",
    "            l.append(b.sample(10).corr(\"spearman\"))\n",
    "\n",
    "tmp_df = pd.concat(l)\n",
    "tmp_df.index.name = \"scorer\"\n",
    "tmp_df = tmp_df.groupby(\"scorer\").mean()\n",
    "tmp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "workers = df.worker.unique()\n",
    "tmp_df2 = []\n",
    "for w1, w2 in product(workers, workers):\n",
    "    if w1 != w2:\n",
    "        tmp_df = df[df.worker.isin([w1, w2])].copy()\n",
    "        tmp_df[\"polid\"] = tmp_df.policy + tmp_df.id\n",
    "        a = tmp_df.groupby(([\"id\", \"policy\"])).count().reset_index()\n",
    "        a = a[a[\"info\"] > 1]\n",
    "        a[\"polid\"] = a.policy + a.id\n",
    "        b = tmp_df[tmp_df.apply(lambda x: (x.policy + x.id) in list(a.polid), axis=1)]\n",
    "        b = b.groupby([\"id\", \"policy\", \"worker\"]).first().reset_index()\n",
    "        b1 = b[b.worker == w1][[\"human_score\", \"polid\"]]\n",
    "        b2 = b[b.worker == w2][[\"human_score\", \"polid\"]]\n",
    "        b = pd.merge(b1, b2, on=\"polid\", how=\"inner\").drop(\"polid\", axis=1)\n",
    "        # print(len(b))\n",
    "        for i in range(100):\n",
    "            if len(b) >= 10:\n",
    "                b = b.sample(10).mean()\n",
    "                tmp_df2.append([b.human_score_x, b.human_score_y])\n",
    "\n",
    "tmp_df2 = pd.DataFrame(tmp_df2)\n",
    "tmp_df2.index.name = \"scorer\"\n",
    "tmp_df = tmp_df2.corr(\"spearman\")\n",
    "tmp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "workers = df.worker.unique()\n",
    "l = []\n",
    "df = df[df.policy.isin(policies)]\n",
    "\n",
    "for w1, w2 in product(workers, workers):\n",
    "    if w1 != w2:\n",
    "        tmp_df = df[df.worker.isin([w1, w2])].copy()\n",
    "        tmp_df[\"polid\"] = tmp_df.policy + tmp_df.id\n",
    "        a = tmp_df.groupby(([\"id\", \"policy\"])).count().reset_index()\n",
    "        a = a[a[\"info\"]>1]\n",
    "        a[\"polid\"] = a.policy + a.id\n",
    "        b = tmp_df[tmp_df.apply(lambda x: (x.policy + x.id) in list(a.polid), axis=1)]\n",
    "        b = b.groupby([\"id\", \"policy\", \"worker\"]).first().reset_index()\n",
    "        b1 = b[b.worker == w1][[\"human_score\", \"polid\"]]\n",
    "        b2 = b[b.worker == w2][[\"human_score\", \"polid\"]]\n",
    "        b = pd.merge(b1, b2, on=\"polid\", how=\"inner\").drop(\"polid\", axis=1)\n",
    "        # print(len(b))\n",
    "        if len(b) >= 10:\n",
    "            # display(b)\n",
    "            for _ in range(100):\n",
    "                l.append(b.sample(10).corr(\"spearman\"))\n",
    "\n",
    "tmp_df = pd.concat(l)\n",
    "tmp_df.index.name = \"scorer\"\n",
    "tmp_df = tmp_df.groupby(\"scorer\").mean()\n",
    "tmp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_score = tmp_df.iloc[0].human_score_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s0[[\"s1\", \"s2\", \"s3\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s0 = s0[[\"s1\", \"s2\", \"s3\"]]\n",
    "# s0.index = clean_target_col\n",
    "s0.index.name = \"\"\n",
    "s0.loc[\"human_score\", :] = [human_score, 0, 0]\n",
    "mask = np.array([[False, False, False]] *9 +  [[False, True, True]])\n",
    "\n",
    "# display(corr)\n",
    "# plot the heatmap\n",
    "plt.figure()\n",
    "\n",
    "ax = sns.heatmap(s0,\n",
    "        # xticklabels=clean_target_col,\n",
    "        yticklabels=clean_target_col,\n",
    "            annot=True, vmin=0.0, vmax=0.6, mask=mask, cbar=False,\n",
    "            cmap=\"coolwarm\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../outputs/plots/exp0b.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s0.index = clean_target_col\n",
    "s0.index.name = \"\"\n",
    "s0.loc[\"Human\", :] = [human_score, \"-\", \"-\"]\n",
    "# Print with 2 decimals printed in latex\n",
    "\n",
    "print(s0.round(2).to_latex(float_format='%.2f'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
