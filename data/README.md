# Revisiting Instruction Fine-tuned Model Evaluation to Guide Industrial Applications


This folder contains the data used in the EMNLP 2023 paper [Revisiting Instruction Fine-tuned Model Evaluation to Guide Industrial Applications](https://arxiv.org/abs/XXX).
Code is available on Github at https://github.com/ManuelFay/IFTEval.

## Abstract 

Instruction Fine-Tuning (IFT) is a powerful paradigm that strengthens the zero-shot capabilities of Large Language Models (LLMs), but in doing so induces new evaluation metric requirements. We show LLM-based metrics to be well adapted to these requirements, and leverage them to conduct an investigation of task-specialization strategies, quantifying the trade-offs that emerge in practical industrial settings. Our findings offer practitioners actionable insights for real-world IFT model deployment.

## Citation

If you use this code for your research, please cite our paper:

```
@article{faysse2023revisiting,
  title={Revisiting Instruction Fine-tuned Model Evaluation to Guide Industrial Applications},
  author={Manuel Faysse, Gautier Viaud, CÃ©line Hudelot, Pierre Colombo},
  journal={EMNLP},
  year={2023}
}
```

## Data

To facilitate data hosting and distribution, datasets are hosted on the HuggingFace hub.
These data include the instruction training sets used for both parts of the paper, the raw results of all experiments, and the final aggregated results.

The datasets are available at the following link:
https://huggingface.co/datasets/manu/IFTEval
